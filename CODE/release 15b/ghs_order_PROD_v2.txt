from cip.cip.framework.connections.spark import spark
from cip.cip.framework.files.read import cnt
#from cip.cip.framework.logging.inboundLogging import aa
import subprocess
from datetime import datetime, timedelta
import pyspark.sql.functions as F

class ghs_order:
    def __init__(self):
        """
        Assigning Variable from config file
        """
        #Target Table (requires adding to config.yaml)
        self.target_db = cnt["Customer_Staging_Mart"]["csm_database"]["landing_database"]
        self.target_table = 'cdd_raw_ghs_order'
        #self.target_table = cnt["Customer_Staging_Mart"]["csm_tables"]["cdd_raw_ghs_order"] #requires adding to config.yaml
        
        #Source Table
        self.source_db = cnt["Datalake"]["source_database"]["sng_database"] #it's confusing that this is called sng_database when other tables are included but this is how it is referred to in config.yaml [gb_mb_secured_dl_tables]
        #self.source_table = cnt["Datalake"]["source_tables"]["ghs_order"] #requires adding to config.yaml
        self.source_table = 'ghs_order'
        
        #self.highwatermark_path = cnt["highwatermark"]["ghs_order_raw_dir"]
        #self.run_path = cnt["run_path"]["root_path"]
        self.run_path = "/user/svc_uk_cust_rdl"
        self.highwatermark_path = '/state/ghs_order_max_date.txt'
        self.highwatermark_raw = \
                spark.read.csv(str(self.run_path) + str(self.highwatermark_path)).rdd.flatMap(lambda x: x).collect()[0]
        self.highwatermark_int1 = datetime.strptime(self.highwatermark_raw, '%Y-%m-%d')
        self.highwatermark_int2 = self.highwatermark_int1 - timedelta(days=3)
        self.highwatermark = self.highwatermark_int2.strftime("%Y-%m-%d")
        
        print(self.highwatermark)

    def run(self):
        print("****************Execution Start****************")

        """
        # for loading first time
        df_output = spark.sql("SELECT * FROM {}.{}".format(self.source_db,self.source_table))
        df_output = df_output.withColumn("load_dt", F.to_date((F.substring(df_output.load_ts, 0, 10))))
        df_output.write.partitionBy("load_dt").mode("overwrite").saveAsTable("{}.{}".format(self.target_db, self.target_table))
        #aa.run(df_output, self.target_db, self.target_table)
        """

        # for loading subsequent data, table should be refreshed with 3 days of data.
        self.highwatermark = datetime.strptime(self.highwatermark, '%Y-%m-%d')
        df_output = spark.sql("SELECT * FROM {}.{}".format(self.source_db,self.source_table))
        df_output = df_output.withColumn("load_dt", F.to_date((F.substring(df_output.load_ts,0,10))))
        df_output = df_output.filter(df_output.load_dt >= self.highwatermark)

        spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")
        spark.conf.set("hive.exec.dynamic.partition", "true")
        spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")
        df_output.write.mode("overwrite").insertInto("{}.{}".format(self.target_db, self.target_table), overwrite=True)

        # Over the high watermark
        new_max_date = df_output.agg(F.max('load_dt')).collect()[0][0]

        proc = subprocess.Popen(
            "echo " + str(new_max_date) + " |" + " hadoop fs -put -f - " + str(self.run_path) + str(
                self.highwatermark_path),
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)

        (output, errors) = proc.communicate()

        print("*******************Execution End*********************")

ts = ghs_order()
ts.run()