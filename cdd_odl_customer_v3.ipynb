{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a567efe-3e16-4a29-9151-13205409425c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating cdd_odl_customer table in ce schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7326f27-5758-4156-8243-4cf37f7c8abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F #import libraries \n",
    "import pandas as pn, functools, operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a9eb84-6960-4fcf-990f-27333fc28c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function to indicate if the account is a test account or valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58318f27-0fa5-4423-8f98-6fa7f1fc4bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def with_column_test_account(inputdf, column_mapping):\n",
    "        \"\"\"check a member if it is test account, and add a column to the input dataframe \"\"\"\n",
    "\n",
    "        # ingest vw_test_account table\n",
    "        vw_test_account_string_table = 'coreprod.clbadm.vw_test_account_string'\n",
    "        vw_test_account_string_sdf = spark.read.table(vw_test_account_string_table)\n",
    "\n",
    "        vw_test_account_string_field_list = vw_test_account_string_sdf.select('field').distinct().toPandas()['field'].to_list()\n",
    "        # Check if all the mapping keys are with EMAIL and FIRST NAME which it would be appear in the csv files\n",
    "        check_keys_is_valid = all(item in column_mapping.keys() for item in vw_test_account_string_field_list)\n",
    "        assert check_keys_is_valid, f'column_mapping keys has to contain the field:{vw_test_account_string_field_list} in coreprod.clbadm.vw_test_account_string'\n",
    "        # Check if all the column values is in the columns\n",
    "        check_values_is_valid = all(item in inputdf.columns for item in column_mapping.values())\n",
    "        assert check_values_is_valid, 'column_mapping values has to exists in the input Spark DataFrame'\n",
    "\n",
    "        # Unpack the Spark DataFrame into List of list (row-oriented)\n",
    "        vw_test_account_string_field_lists = vw_test_account_string_sdf.select(\"string\", \"field\").toPandas().values.tolist()\n",
    "\n",
    "        # Map the field to the PySpark Like Expression\n",
    "        # Replacing [_] with \\r (Different Syntax for T-SQL and Databricks SQL)\n",
    "        \n",
    "        rules_field_in_pyspark = [\n",
    "            F.col(column_mapping[field]).like(rules.replace('[_]', r'\\_')) \n",
    "            for rules, field in vw_test_account_string_field_lists\n",
    "            ]\n",
    "        rules_field_in_pyspark_reduced_or = functools.reduce(operator.or_, rules_field_in_pyspark, F.lit(False))\n",
    "\n",
    "        # initiate the is_test_account column\n",
    "        inputdf = inputdf.withColumn('is_test_account', F.lit('0'))\n",
    "        outputdf = inputdf.withColumn('is_test_account', \n",
    "                                            F.when(rules_field_in_pyspark_reduced_or, '1')\n",
    "                                            .otherwise(F.col('is_test_account')))\n",
    "        return outputdf\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728c875f-6031-4dd8-8f5f-3398baaddcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function to create tnc_accepted_at_sng column from kiosk and mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9caf44-f513-426d-a5b7-9e571ebb5fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  \n",
    "def getSNGData(df):\n",
    "    df_output = df.withColumn(\"tnc_accepted_at_sng\",\n",
    "        F.when(F.col(\"tnc_accepted_at_sng_kiosk\") < F.col(\"tnc_accepted_at_sng_mobile\"),\n",
    "                F.col(\"tnc_accepted_at_sng_kiosk\")).otherwise(F.col(\"tnc_accepted_at_sng_mobile\"))\n",
    "    )\n",
    "\n",
    "    return (df_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c255692-224a-4ff8-82cc-ab4bd6f45ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895cf8bd-7721-46b8-b47f-8c9683748456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SinglProflCustomer:\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Ingesting data ...\")\n",
    "        #create df of all data to be joined from source \n",
    "        self.dim_cust = (spark.sql(\"\"\"\n",
    "            SELECT NULL AS unified_cust_id,\n",
    "                   bk_crm_customer_id AS crm_id,\n",
    "                   bk_singl_profl_id AS singl_profl_id,\n",
    "                   secondary_loginid AS scndry_login_id,\n",
    "                   account_status,\n",
    "                   prefix,\n",
    "                   first_name AS first_nm,\n",
    "                   initcap(first_name) as contactable_first_nm,\n",
    "                   last_name AS last_nm,\n",
    "                   email,\n",
    "                   is_guest_customer AS guest_ind,\n",
    "                   registration_date,\n",
    "                   registration_channel,\n",
    "                   date_of_birth,\n",
    "                   NULL AS gdpr_del_ind,\n",
    "                   suspend_status,\n",
    "                   suspend_reason,\n",
    "                   suspend_start_date AS suspend_ts\n",
    "            FROM coreprod.infomart.dim_customer\n",
    "            \n",
    "             \"\"\")\n",
    "        )  # Filter by date when loading history \n",
    "#--where registration_date between '2021-10-01' and '2024-11-30'\n",
    "        self.clean_cust_data = with_column_test_account(self.dim_cust, {\n",
    "                    \"EMAIL\":\"email\",\n",
    "                    'FIRST_NAME': 'first_nm'\n",
    "                })\n",
    "        \n",
    "        self.cust_agg_attr = spark.sql(\"\"\"select measure_nm, channel_cd\n",
    "                            , singl_profl_userid\n",
    "                            , measure_val\n",
    "                            , ROW_NUMBER() over (partition by singl_profl_userid, channel_cd order by measure_val desc) rn\n",
    "                            from coreprod.gb_mb_secured_aggregate_dl_tables.cust_agg_attr where measure_nm = 'LAST_LOGIN_DATE'\"\"\")        \n",
    "        self.cust_agg_attr = self.cust_agg_attr.filter((F.col(\"rn\") == '1') & F.col(\"channel_cd\").isNotNull()).drop(\"rn\")\n",
    "        \n",
    "        self.channels = [\"GROCERY\", \"SNGKIOSK\", \"SNGMOBILE\", \"GEORGE\", \"ASDA\", \"GIFTCARDS\", \"LOYALTY\"]\n",
    "\n",
    "        self.df_login = {}\n",
    "\n",
    "        for channel in self.channels:\n",
    "            self.df_login[channel] = self.cust_agg_attr.filter(\n",
    "                (F.upper(F.col(\"channel_cd\")) == (channel).upper())) \\\n",
    "                .select(F.col('singl_profl_userid'), #.alias(f'tnc_login_{channel.lower()}_userid'),\n",
    "                        F.col('measure_val').alias(f'last_login_at_{channel.lower()}'))\n",
    "\n",
    "\n",
    "        self.logins = (self.df_login.get(\"GROCERY\").join(self.df_login.get(\"SNGKIOSK\"), on='singl_profl_userid', how='leftouter')\n",
    "                       .join(self.df_login.get(\"SNGMOBILE\"), on='singl_profl_userid', how='leftouter')\n",
    "                       .join(self.df_login.get(\"GEORGE\"), on='singl_profl_userid', how='leftouter')\n",
    "                       .join(self.df_login.get(\"ASDA\"), on='singl_profl_userid', how='leftouter')\n",
    "                       .join(self.df_login.get(\"GIFTCARDS\"), on='singl_profl_userid', how='leftouter')\n",
    "                       .join(self.df_login.get(\"LOYALTY\"), on='singl_profl_userid', how='leftouter'))\n",
    "        \n",
    "        self.logins.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"custanwo.ce.lastLoginData\")\n",
    "\n",
    "        self.logins = spark.sql(\"select * from custanwo.ce.lastLoginData\")\n",
    "        \n",
    "        self.loyalty_acct = spark.sql(\"\"\"select distinct wallet_id, singl_profl_id as la_spid from coreprod.eagleeye.loyalty_acct\"\"\")\n",
    "        \n",
    "        self.df_ctc = spark.sql(\"\"\" SELECT distinct bk_singl_profl_id, lastmodifieddate, servicename__c from coreprod.infomart.fact_customer_terms_condition where servicename__c is not null \n",
    "                             \"\"\")\n",
    "        \n",
    "        self.distinct_ids = self.clean_cust_data.filter(F.col(\"singl_profl_id\").isNotNull()).select(\"singl_profl_id\").distinct().withColumnRenamed(\"singl_profl_id\", \"bk_singl_profl_id\")\n",
    "        self.distinct_ids.createOrReplaceTempView(\"distinct_ids\")\n",
    "\n",
    "        service_names = spark.sql(\"select distinct servicename__c from coreprod.infomart.fact_customer_terms_condition where servicename__c is not null\").toPandas().values.tolist()\n",
    "        service_nameslst = [x[0] for x in service_names]\n",
    "\n",
    "\n",
    "        self.df_tnc = {}\n",
    "\n",
    "        for service_name in service_nameslst:\n",
    "            #print(service_name)\n",
    "            self.df_tnc[service_name] = self.df_ctc.groupBy(\"bk_singl_profl_id\", \"servicename__c\").agg(F.min(\"lastmodifieddate\").alias(\"lastmodifieddate\")).filter(F.col(\"servicename__c\") == service_name) \\\n",
    "                .select(F.col('bk_singl_profl_id'),#.alias(f'tnc_{service_name.lower().replace(\" \", \"_\")}_userid'),\n",
    "                        F.col('lastmodifieddate').alias(f'tnc_accepted_at_{service_name.lower().replace(\" \", \"_\")}'))\n",
    "            self.df_tnc[service_name].createOrReplaceTempView(service_name.lower().replace(\" \", \"_\"))    \n",
    "        \n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct_ids.bk_singl_profl_id, groceries.tnc_accepted_at_groceries\n",
    "                                         from distinct_ids left join (select distinct bk_singl_profl_id, tnc_accepted_at_groceries from groceries)\n",
    "                                         groceries on distinct_ids.bk_singl_profl_id = groceries.bk_singl_profl_id \n",
    "                                         \n",
    "                                         \"\"\")\n",
    "        print(\"spids and groceries joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id\n",
    "                                         , servicetable.tnc_accepted_at_groceries\n",
    "                                         , sng_kiosk.tnc_accepted_at_sng_kiosk \n",
    "                                                          from servicetable \n",
    "                                                          left join (select distinct bk_singl_profl_id, tnc_accepted_at_sng_kiosk from sng_kiosk) sng_kiosk \n",
    "                                                          on servicetable.bk_singl_profl_id = sng_kiosk.bk_singl_profl_id \n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries and kiosk joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id\n",
    "                                         ,servicetable.tnc_accepted_at_groceries\n",
    "                                         ,servicetable.tnc_accepted_at_sng_kiosk\n",
    "                                         ,sng_mobile.tnc_accepted_at_sng_mobile \n",
    "                                         from servicetable left join (select bk_singl_profl_id, tnc_accepted_at_sng_mobile from sng_mobile ) sng_mobile on servicetable.bk_singl_profl_id = sng_mobile.bk_singl_profl_id\n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries, kiosk and mobile joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id\n",
    "                                         ,servicetable.tnc_accepted_at_groceries\n",
    "                                         ,servicetable.tnc_accepted_at_sng_kiosk\n",
    "                                         ,servicetable.tnc_accepted_at_sng_mobile\n",
    "                                         ,george.tnc_accepted_at_george \n",
    "                                         from servicetable left join (select bk_singl_profl_id, tnc_accepted_at_george from george) george on servicetable.bk_singl_profl_id = george.bk_singl_profl_id\n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries, kiosk, mobile and george joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id\n",
    "                                         , servicetable.tnc_accepted_at_groceries\n",
    "                                         , servicetable.tnc_accepted_at_sng_kiosk\n",
    "                                         , servicetable.tnc_accepted_at_sng_mobile\n",
    "                                         , servicetable.tnc_accepted_at_george\n",
    "                                         , gift_card.tnc_accepted_at_gift_card\n",
    "                                         from servicetable left join (select distinct bk_singl_profl_id, tnc_accepted_at_gift_card from gift_card) gift_card on servicetable.bk_singl_profl_id = gift_card.bk_singl_profl_id\n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries, kiosk, mobile, george and gift card joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id\n",
    "                                         , servicetable.tnc_accepted_at_groceries\n",
    "                                         , servicetable.tnc_accepted_at_sng_kiosk\n",
    "                                         , servicetable.tnc_accepted_at_sng_mobile\n",
    "                                         , servicetable.tnc_accepted_at_george\n",
    "                                         , servicetable.tnc_accepted_at_gift_card\n",
    "                                         , asda_rewards.tnc_accepted_at_asda_rewards\n",
    "                                         from servicetable left join (select distinct bk_singl_profl_id, tnc_accepted_at_asda_rewards from asda_rewards) asda_rewards on servicetable.bk_singl_profl_id = asda_rewards.bk_singl_profl_id\n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries, kiosk, mobile, george, gift card and asda rewards joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id, servicetable.tnc_accepted_at_groceries, servicetable.tnc_accepted_at_sng_kiosk, servicetable.tnc_accepted_at_sng_mobile, servicetable.tnc_accepted_at_george, servicetable.tnc_accepted_at_gift_card, servicetable.tnc_accepted_at_asda_rewards, credit_card.tnc_accepted_at_credit_card\n",
    "                                         from servicetable left join (select distinct bk_singl_profl_id, tnc_accepted_at_credit_card from credit_card) credit_card on servicetable.bk_singl_profl_id = credit_card.bk_singl_profl_id\n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries, kiosk, mobile, george, gift card, asda rewards and credit card joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "\n",
    "        self.df_servicenames = spark.sql(\"\"\"select distinct servicetable.bk_singl_profl_id, servicetable.tnc_accepted_at_groceries, servicetable.tnc_accepted_at_sng_kiosk, servicetable.tnc_accepted_at_sng_mobile, servicetable.tnc_accepted_at_george, servicetable.tnc_accepted_at_gift_card, servicetable.tnc_accepted_at_asda_rewards, servicetable.tnc_accepted_at_credit_card, asda_mobile.tnc_accepted_at_asda_mobile\n",
    "                                         from servicetable left join (select distinct bk_singl_profl_id, tnc_accepted_at_asda_mobile from asda_mobile) asda_mobile on servicetable.bk_singl_profl_id = asda_mobile.bk_singl_profl_id\n",
    "                                         \"\"\")\n",
    "        print(\"spids, groceries, kiosk, mobile, george, gift card, asda rewards, credit card and asda mobile joined \", self.df_servicenames.count())\n",
    "        self.df_servicenames.createOrReplaceTempView(\"servicetable\")\n",
    "                                        \n",
    "                                         #sng_mobile on distinct_ids.bk_singl_profl_id = sng_mobile.bk_singl_profl_id left join\n",
    "                                         #george on distinct_ids.bk_singl_profl_id = george.bk_singl_profl_id left join\n",
    "                                         #gift_card on distinct_ids.bk_singl_profl_id = gift_card.bk_singl_profl_id\n",
    "                                        # asda_rewards on distinct_ids.bk_singl_profl_id = asda_rewards.bk_singl_profl_id left #join\n",
    "                                        # credit_card on distinct_ids.bk_singl_profl_id = credit_card.bk_singl_profl_id left join\n",
    "                                        # asda_mobile on distinct_ids.bk_singl_profl_id = asda_mobile.bk_singl_profl_id                                        \n",
    "                                         \n",
    "        self.df_servicenames.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(\"custanwo.ce.serviceNameData\")\n",
    "\n",
    "        self.df2_servicenames = spark.sql(\"select * from custanwo.ce.serviceNameData\")\n",
    "        \n",
    "    def run(self):\n",
    "        print(\"** Execution Start **\")\n",
    "        \n",
    "        #spids = self.clean_cust_data.select(\"singl_profl_id\").distinct()\n",
    "        #self.logins = self.logins.join(spids, on=spids.singl_profl_id == self.logins.singl_profl_userid, how='inner').drop(\"singl_profl_id\")\n",
    "        \n",
    "        #self.loyalty_acct = self.loyalty_acct.join(spids, on=spids.singl_profl_id == self.loyalty_acct.la_spid, how='inner').drop(\"singl_profl_id\")\n",
    "\n",
    "        df1 = self.clean_cust_data.join(self.logins, on=self.clean_cust_data.singl_profl_id == self.logins.singl_profl_userid, how='left')\n",
    "\n",
    "        df1.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"custanwo.ce.loginData_joinedCustdata\")\n",
    "\n",
    "        df2 = df1.join(self.df2_servicenames, on=df1.singl_profl_id == self.df2_servicenames.bk_singl_profl_id, how='left').drop(\"bk_singl_profl_id\")\n",
    "    \n",
    "        df3 = df2.join(self.loyalty_acct, on=df2.singl_profl_id == self.loyalty_acct.la_spid, how='left')\n",
    "\n",
    "        df4 = df3.withColumnsRenamed({\"tnc_accepted_at_asda_rewards\": \"tnc_accepted_at_loyalty\", \"last_login_at_sngkiosk\": \"last_login_at_sng_kiosk\", \"last_login_at_sngmobile\": \"last_login_at_sng_mobile\", \"last_login_at_giftcards\":\"last_login_at_gift_card\", \"is_test_account\":\"test_account_ind\"})\n",
    "\n",
    "        df5 = getSNGData(df4)\n",
    "\n",
    "        df = df5.select('unified_cust_id', 'crm_id','singl_profl_id','wallet_id','scndry_login_id'\n",
    "                       ,'account_status','prefix','first_nm','contactable_first_nm','last_nm','email','guest_ind','registration_date','registration_channel','date_of_birth','gdpr_del_ind','suspend_status'\n",
    "                        ,'suspend_reason','suspend_ts','tnc_accepted_at_groceries','tnc_accepted_at_sng'\n",
    "                        ,'tnc_accepted_at_sng_kiosk','tnc_accepted_at_sng_mobile','tnc_accepted_at_george'\n",
    "                        ,'tnc_accepted_at_gift_card','tnc_accepted_at_loyalty','tnc_accepted_at_credit_card'\n",
    "                        ,'tnc_accepted_at_asda_mobile','last_login_at_grocery','last_login_at_sng_kiosk'\n",
    "                       ,'last_login_at_sng_mobile','last_login_at_george','last_login_at_asda','last_login_at_gift_card','last_login_at_loyalty','test_account_ind').distinct()\n",
    "\n",
    "        df = df.withColumn(\"unified_cust_id\", F.lit(None).cast(\"string\")).withColumn(\"gdpr_del_ind\", F.lit(None).cast(\"string\"))\n",
    "        df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"custanwo.ce.cdd_odl_customer\")\n",
    "        \n",
    "        print(\"** Execution End **\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a594938-5a3e-4605-98d1-1b731292c2f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4b4d50-2971-4967-a41e-2d9af75e017d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aa = SinglProflCustomer()\n",
    "aa.run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 43774812537856,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "cdd_odl_customer_v3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
